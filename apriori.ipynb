{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d76dbc86-465f-406a-a4d3-a0ddafac5aa1",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Apriori\n",
    "This repository contains a basic implementation of the (randomized) Apriori algorithm for mining frequent item sets using the market-basket model. We compare the effectiveness and efficience of the two algorithms by running them on real data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bf535ab7-f70d-402f-a8d0-215910d1aeeb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60f75ee7-7604-4697-8603-d8851fabe1b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from src.apriori import apriori"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd14856-5723-4608-9a1a-6067b71c6120",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. Load Data\n",
    "First, let's load some testing data sets. The input format of the data is one line per basket, and each line contains the IDs of the items in the basket separated by space. We can assume that the IDs are 0,1,2,... More specifically, let's consider the following two data sets:\n",
    "- *Retail*: A retail market basket dataset supplied by a anonymous Belgian retail supermarket store (small data set).\n",
    "- *WebDocs*: A huge real-life transactional dataset built from a spidered collection of web html documents (huge data set).\n",
    "\n",
    "We download these data sets from the web first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d399e3d0-e92d-4c7d-9326-f43d06e4dee6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "execute_download: bool = False # change this line for download\n",
    "if execute_download:\n",
    "    # retails data set\n",
    "    with open('./data/retail.dat.gz', 'wb') as outfile:\n",
    "        content = requests.get('http://fimi.uantwerpen.be/data/retail.dat.gz', stream=True).content\n",
    "        outfile.write(content)\n",
    "\n",
    "    # webdocs data set\n",
    "    with open('./data/webdocs.dat.gz', 'wb') as outfile:\n",
    "        content = requests.get('http://fimi.uantwerpen.be/data/webdocs.dat.gz', stream=True).content\n",
    "        outfile.write(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c8c56f1-cb58-4925-81b7-9110d0837af5",
   "metadata": {},
   "source": [
    "Go ahead and unzip these files using the preferred method on your OS such that you have the raw `.dat` files.\n",
    "\n",
    "The big webdocs dataset shows why we often prefer approximate results: Here, the standard algorithm may be very slow, whereas a good implementation (without any smart techniques) of the randomized algorithm should be able to produce reasonably good results much faster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d54838-3727-4e9a-bacf-f6b39b19a787",
   "metadata": {},
   "source": [
    "## 2. Implementation Details\n",
    "We implement two versions of the Apriori algorithm.\n",
    "\n",
    "### 2.1. Standard Apriori\n",
    "First, let's consider the standard version. The algorithm makes multiple passes over the data. \n",
    "- In the first pass, it counts the frequency of all items; then, it discards those whose frequency is below the threshold $t$.\n",
    "- The set of frequent items is then used to generate the candidate pairs. In a second pass, the actual frequent pairs are computed.\n",
    "- The sets of frequent items and frequent pairs is used to generate the candidate triples. \n",
    "\n",
    "Generally speaking, after $k$ passes over the data, the algorithm maintains the set of frequent $j$-itemsets (of size $j$), for every $j\\in\\{1,2,...,k\\}$, and uses the information on frequent items and frequent $k$-itemsets to generate the candidate $k + 1$-itemsets to be verified in the next pass. The algorithm stops when no frequent k-itemsets are found.\n",
    "\n",
    "The algorithm takes in input the path of the dataset on disk, and an integer threshold $t$. It returns a dictionary that has frequent itemsets as keys with their frequency as values.\n",
    "\n",
    "### 2.2. Randomized Apriori\n",
    "Apriori itself can take very long. For this reason, we also consider a randomized sample-based version of the algorithm that works as follows.\n",
    "\n",
    "1. The algorithm takes in input the path of the dataset on disk, an integer threshold $t$, a sampling probability $p$, and a boolean $f$ that will allow for an extra pass over the data to remove false positives.\n",
    "2. The first step of the algorithm is that of creating the sample: read the dataset once, sample each basket with probability $p$ and append it into a list $s$ to be maintained in main memory.\n",
    "3. Run Apriori on $s$. Differently from the previous implementation, here we will proceed in passes over $s$ and will not read from disk. With a good implementation it might be possible to re-use most of the code written above.\n",
    "4. If $f$ is true, perform an extra pass over the data on disk to remove false positives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b7143e64-88b6-4f87-8814-53fd0c698edd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pass 1 finished.\n",
      "Pass 2 finished.\n",
      "Pass 3 finished.\n",
      "Pass 4 finished.\n"
     ]
    }
   ],
   "source": [
    "output = apriori(file_path='./data/retail.dat', t=700)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7702dec4-8b25-49bc-846c-09d566b168d6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "249"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
